{
  "cells": [
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# Women in Tech - Cognitive services workshop\n\nThis workshop is to acquaint you with a very small subselection of Microsoft Cognitive Services. These services make it very easy to leverage the power of AI in your software solutions. You can find more information on <a href=\"https://azure.microsoft.com/en-us/services/cognitive-services/\">the official webpage</a>.\n\nThe examples in this workshop come from this Microsoft github: https://github.com/microsoft/cognitive-services-notebooks\n\nWe are going to use Azure Notebooks for this. With Azure Notebooks you are able to run Jupyter notebooks from the cloud instead of your desktop. Read more on Azure Notebooks <a href=\"https://notebooks.azure.com/\">here</a>\n\n## Prerequisites\n* Know what Python is\n* Laptop or other device\n* Working internet connection\n\n\n## Contents\n\nFor this workshop we are going to focus on some of the Computer Vision Cognitive Services. We are going to accomplish the following tasks:\n* [Analyze an image](#AnalyzeImage)\n* [Use a domain-specific Model](#DomainSpecificModel)\n* [Intelligently generate a thumbnail](#GetThumbnail)\n* [Detect and extract printed text from an image](#OCR)\n* [Detect and extract handwritten text from an image](#RecognizeText)\n\nThese tasks are pretty straight-forward for most humans, but are very hard to accomplish by writing code. By using the Cognitive Services in Azure, we can accomplish these tasks by just calling into an API. The only thing you need is a subscription key you can get from the Azure portal. Just add a new 'Cognitive Services' resource and copy the subscription key provided to you. \n\nTo save time and allow everyone to participate, I have already created this resource and provided the key in this notebook. This key will however be deleted somewhere in the near future when we are done with the workshop.\n\nIf you want to keep using the cognitive services after this workshop, just follow <a href=\"https://docs.microsoft.com/nl-nl/azure/cognitive-services/cognitive-services-apis-create-account\">these instructions</a>"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Setting up our notebook with a Cognitive Services subscription key"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "We assign the Cognitive Services subscription key to a local variable:"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "subscription_key = \"ff7942ef54fe4146bf49094999093c01\"\nassert subscription_key",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Then we assign the base api url for the Vision API to a local variable as well:"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "vision_base_url = \"https://westeurope.api.cognitive.microsoft.com/vision/v2.0/\"",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Since the Cognitive Services resource associated with that subscription key is located in the West Europe region, the end point has to point to the correct region.\n\nWe need to import some standard libraries to make http calls, display and manipulate the images:"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import requests\nfrom PIL import Image \nfrom IPython.display import Image as Img\nimport matplotlib.pyplot as plt\nfrom io import BytesIO",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Analyze an image with Computer Vision API using Python <a name=\"AnalyzeImage\"> </a>\n\nWith the [Analyze Image method](https://westcentralus.dev.cognitive.microsoft.com/docs/services/5adf991815e1060e6355ad44/operations/56f91f2e778daf14a499e1fa), you can extract visual features based on image content. You can upload an image or specify an image URL and choose which features to return, including:\n* A detailed list of tags related to the image content.\n* A description of image content in a complete sentence.\n* The coordinates, gender, and age of any faces contained in the image.\n* The ImageType (clip art or a line drawing).\n* The dominant color, the accent color, or whether an image is black & white.\n* The category defined in this [taxonomy](https://docs.microsoft.com/azure/cognitive-services/computer-vision/category-taxonomy).\n* Does the image contain adult or sexually suggestive content?\n\n### Analyze an image "
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "The image analysis URL looks like the following (see REST API docs [here](https://westcentralus.dev.cognitive.microsoft.com/docs/services/5adf991815e1060e6355ad44/operations/56f91f2e778daf14a499e1fa)):\n<code>\nhttps://[location].api.cognitive.microsoft.com/vision/v2.0/<b>analyze</b>[?visualFeatures][&details][&language]\n</code>"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "vision_analyze_url = vision_base_url + \"analyze\"",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "To begin analyzing an image, set `image_url` to the URL of any image that you want to analyze. I have included a couple, but feel free to inject any image URL which still is appropriate for this workshop."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "image_url = \"https://upload.wikimedia.org/wikipedia/commons/thumb/1/12/Broadway_and_Times_Square_by_night.jpg/450px-Broadway_and_Times_Square_by_night.jpg\"\n# image_url = \"https://images.pexels.com/photos/2755165/pexels-photo-2755165.jpeg?auto=compress&cs=tinysrgb&dpr=2&h=750&w=1260\"\n# image_url = \"https://images.pexels.com/photos/2986509/pexels-photo-2986509.jpeg?auto=compress&cs=tinysrgb&dpr=2&h=750&w=1260\"\n# image_url = \"https://images.pexels.com/photos/2765872/pexels-photo-2765872.jpeg?auto=compress&cs=tinysrgb&dpr=2&h=750&w=1260\"\n# image_url = \"https://images.pexels.com/photos/3183132/pexels-photo-3183132.jpeg?auto=compress&cs=tinysrgb&dpr=2&h=750&w=1260\"\n# image_url = \"https://images.pexels.com/photos/2986374/pexels-photo-2986374.jpeg?auto=compress&cs=tinysrgb&dpr=2&h=750&w=1260\"\n# image_url = \"https://images.pexels.com/photos/3027216/pexels-photo-3027216.jpeg?auto=compress&cs=tinysrgb&dpr=2&h=750&w=1260\"",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "To show a sample of the image, just run the code block below."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "Img(url=image_url, width=250)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "The following block uses the `requests` library in Python to call out to the Computer Vision `analyze` API and return the results as a JSON object. The API key is passed in via the `headers` dictionary and the types of features to recognize via the `params` dictionary. To see the full list of options that can be used, refer to the [REST API](https://westcentralus.dev.cognitive.microsoft.com/docs/services/5adf991815e1060e6355ad44/operations/56f91f2e778daf14a499e1fa) documentation for image analysis."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "headers  = {'Ocp-Apim-Subscription-Key': subscription_key }\nparams   = {'visualFeatures': 'Categories,Description,Color'}\ndata     = {'url': image_url}\nresponse = requests.post(vision_analyze_url, headers=headers, params=params, json=data)\nresponse.raise_for_status()\nanalysis = response.json()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "The `analysis` object contains various fields that describe the image. The most relevant caption for the image can be obtained from the `descriptions` property. If it has no clue, it will throw an error here which is not handled gracefully."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "image_caption = analysis[\"description\"][\"captions\"][0][\"text\"].capitalize()\nprint(image_caption)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Use a domain-specific model <a name=\"DomainSpecificModel\"> </a>\n\nA [domain-specific model](https://westcentralus.dev.cognitive.microsoft.com/docs/services/5adf991815e1060e6355ad44/operations/56f91f2e778daf14a499e1fd)  is a model trained to identify a specific set of objects in an image.  The two domain-specific models that are currently available are _celebrities_ and _landmarks_. \n\nTo view the list of domain-specific models supported, you can make the following request against the service."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "model_url = vision_base_url + \"models\"\nheaders   = {'Ocp-Apim-Subscription-Key': subscription_key}\nmodels    = requests.get(model_url, headers=headers).json()\n[model[\"name\"] for model in models[\"models\"]]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Landmark identification\nTo begin using the domain-specific model for landmarks, set `image_url` to point to an image to be analyzed."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "image_url = \"https://upload.wikimedia.org/wikipedia/commons/f/f6/Bunker_Hill_Monument_2005.jpg\"\n# image_url = \"https://images.pexels.com/photos/1461974/pexels-photo-1461974.jpeg?auto=compress&cs=tinysrgb&dpr=2&h=750&w=1260\"\n# image_url = \"https://images.pexels.com/photos/1141853/pexels-photo-1141853.jpeg?auto=compress&cs=tinysrgb&dpr=2&h=750&w=1260\"\n# image_url = \"https://images.pexels.com/photos/1448136/pexels-photo-1448136.jpeg?auto=compress&cs=tinysrgb&dpr=2&h=750&w=1260\"\n# image_url = \"https://images.pexels.com/photos/753339/pexels-photo-753339.jpeg?auto=compress&cs=tinysrgb&dpr=2&h=750&w=1260\"\n# image_url = \"https://images.pexels.com/photos/64271/queen-of-liberty-statue-of-liberty-new-york-liberty-statue-64271.jpeg?auto=compress&cs=tinysrgb&dpr=2&h=750&w=1260\"",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "Img(url=image_url, width=250)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "The service endpoint to analyze images for landmarks can be constructed as follows:"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "landmark_analyze_url = vision_base_url + \"models/landmarks/analyze\"\nprint(landmark_analyze_url)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "The image in `image_url` can now be analyzed for any landmarks. The identified landmark is stored in `landmark_name`."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "headers  = {'Ocp-Apim-Subscription-Key': subscription_key}\nparams   = {'model': 'landmarks'}\ndata     = {'url': image_url}\nresponse = requests.post(landmark_analyze_url, headers=headers, params=params, json=data)\nresponse.raise_for_status()\n\nanalysis      = response.json()\nassert analysis[\"result\"][\"landmarks\"] is not []\n\nlandmark_name = analysis[\"result\"][\"landmarks\"][0][\"name\"].capitalize()\nprint(landmark_name)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Celebrity identification\nAlong the same lines, the domain-specific model for identifying celebrities can be invoked as shown next. First set `image_url` to point to the image of a celebrity."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "image_url = \"https://upload.wikimedia.org/wikipedia/commons/d/d9/Bill_gates_portrait.jpg\"\n#image_url = \"https://www.cheatsheet.com/wp-content/uploads/2019/11/Brad-Pitt.jpg\"\n#image_url = \"https://www.thesun.co.uk/wp-content/uploads/2018/07/NINTCHDBPICT0004161277131.jpg\"\n#image_url = \"https://pmcvariety.files.wordpress.com/2019/10/shutterstock_editorial_10435445et.jpg?w=1000&h=563&crop=1\"\n#image_url = \"https://www.grammy.com/sites/com/files/styles/image_landscape_hero/public/edsheeran-hero-478072284.jpg?itok=WFSpwjin\"\n#image_url = \"https://www.oxy.edu/sites/default/files/styles/banner_image/public/top-level-news/obama-scholars-news.jpg?itok=eOKS79MY\"",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "The service endpoint for detecting celebrity images can be constructed as follows:"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "celebrity_analyze_url = vision_base_url + \"models/celebrities/analyze\"\nprint(celebrity_analyze_url)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Next, the image in `image_url` can be analyzed for celebrities"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "headers  = {'Ocp-Apim-Subscription-Key': subscription_key}\nparams   = {'model': 'celebrities'}\ndata     = {'url': image_url}\nresponse = requests.post(celebrity_analyze_url, headers=headers, params=params, json=data)\nresponse.raise_for_status()\n\nanalysis = response.json()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "print(analysis)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "The following lines of code extract the name and bounding box for one of the celebrities found:"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "assert analysis[\"result\"][\"celebrities\"] is not []\ncelebrity_info = analysis[\"result\"][\"celebrities\"][0]\ncelebrity_name = celebrity_info[\"name\"]\ncelebrity_face = celebrity_info[\"faceRectangle\"]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Next, this information can be overlaid on top of the original image using the following lines of code:"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from matplotlib.patches import Rectangle\n\nplt.figure(figsize=(5,5))\n\nimage  = Image.open(BytesIO(requests.get(image_url).content))\nax     = plt.imshow(image, alpha=0.6)\norigin = (celebrity_face[\"left\"], celebrity_face[\"top\"])\np      = Rectangle(origin, celebrity_face[\"width\"], celebrity_face[\"height\"], \n                   fill=False, linewidth=2, color='b')\nax.axes.add_patch(p)\nplt.text(origin[0], origin[1], celebrity_name, fontsize=20, weight=\"bold\", va=\"bottom\")\n_ = plt.axis(\"off\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Get a thumbnail with Computer Vision API <a name=\"GetThumbnail\"> </a>\n\nUse the [Get Thumbnail method](https://westcentralus.dev.cognitive.microsoft.com/docs/services/5adf991815e1060e6355ad44/operations/56f91f2e778daf14a499e1fb) to crop an image based on its region of interest (ROI) to the height and width you desire. The aspect ratio you set for the thumbnail can be different from the aspect ratio of the input image.\n\nTo generate the thumbnail for an image, first set `image_url` to point to its location. "
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "image_url = \"https://upload.wikimedia.org/wikipedia/commons/9/94/Bloodhound_Puppy.jpg\"\nImg(url=image_url, width=250)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "The service endpoint to generate the thumbnail can be constructed as follows:"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "thumbnail_url = vision_base_url + \"generateThumbnail\"\nprint(thumbnail_url)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Next, a 50-by-50 pixel thumbnail for the image can be generated by calling this service endpoint."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "headers  = {'Ocp-Apim-Subscription-Key': subscription_key}\nparams   = {'width': '50', 'height': '50','smartCropping': 'true'}\ndata     = {'url': image_url}\nresponse = requests.post(thumbnail_url, headers=headers, params=params, json=data)\nresponse.raise_for_status()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "You can verify that the thumbnail is indeed 50-by-50 pixels using the Python Image Library."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from PIL import Image as PImage\nthumbnail = PImage.open(BytesIO(response.content))\nprint(\"Thumbnail is {0}-by-{1}\".format(*thumbnail.size))\nthumbnail",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Optical character recognition (OCR) with Computer Vision API <a name=\"OCR\"> </a>\n\nUse the [Optical Character Recognition (OCR) method](https://westcentralus.dev.cognitive.microsoft.com/docs/services/5adf991815e1060e6355ad44/operations/56f91f2e778daf14a499e1fc) to synchronously detect printed text in an image and extract recognized characters into a machine-usable character stream.\n\nTo illustrate the OCR API, set `image_url` to point to the text to be recognized."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "image_url = \"https://upload.wikimedia.org/wikipedia/commons/thumb/a/af/Atomist_quote_from_Democritus.png/338px-Atomist_quote_from_Democritus.png\"\nImg(url=image_url, width=250)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "The service endpoint for OCR for your region can be constructed as follows:"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "ocr_url = vision_base_url + \"ocr\"\nprint(ocr_url)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Next, you can call into the OCR service to get the text that was recognized along with bounding boxes. In the parameters shown, `\"language\": \"unk\"` automatically detects the language in the text and `\"detectOrientation\": \"true\"` automatically aligns the image. For more information, see the [REST API documentation](https://westcentralus.dev.cognitive.microsoft.com/docs/services/5adf991815e1060e6355ad44/operations/56f91f2e778daf14a499e1fc)."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "headers  = {'Ocp-Apim-Subscription-Key': subscription_key}\nparams   = {'language': 'unk', 'detectOrientation ': 'true'}\ndata     = {'url': image_url}\nresponse = requests.post(ocr_url, headers=headers, params=params, json=data)\nresponse.raise_for_status()\n\nanalysis = response.json()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "The word bounding boxes and text from the results of analysis can be extracted using the following lines of code:"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "line_infos = [region[\"lines\"] for region in analysis[\"regions\"]]\nword_infos = []\nfor line in line_infos:\n    for word_metadata in line:\n        for word_info in word_metadata[\"words\"]:\n            word_infos.append(word_info)\nword_infos",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Finally, the recognized text can be overlaid on top of the original image using the `matplotlib` library."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "plt.figure(figsize=(5,5))\n\nimage  = Image.open(BytesIO(requests.get(image_url).content))\nax     = plt.imshow(image, alpha=0.5)\nfor word in word_infos:\n    bbox = [int(num) for num in word[\"boundingBox\"].split(\",\")]\n    text = word[\"text\"]\n    origin = (bbox[0], bbox[1])\n    patch  = Rectangle(origin, bbox[2], bbox[3], fill=False, linewidth=2, color='y')\n    ax.axes.add_patch(patch)\n    plt.text(origin[0], origin[1], text, fontsize=20, weight=\"bold\", va=\"top\")\n_ = plt.axis(\"off\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Text recognition with Computer Vision API <a name=\"RecognizeText\"> </a>\n\nUse the [Recognize Text method](https://westcentralus.dev.cognitive.microsoft.com/docs/services/5adf991815e1060e6355ad44/operations/587f2c6a154055056008f200) to asynchronously detect handwritten or printed text in an image and extract recognized characters into a machine-usable character stream.\n\nSet `image_url` to point to the image to be recognized."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "image_url = \"https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Cursive_Writing_on_Notebook_paper.jpg/800px-Cursive_Writing_on_Notebook_paper.jpg\"\nImg(url=image_url, width=350)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "The service endpoint for the text recognition service can be constructed as follows:"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "text_recognition_url = vision_base_url + \"recognizeText\"\nprint(text_recognition_url)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "The handwritten text recognition service can be used to recognize the text in the image. In the `params` dictionary, you can set `mode` to `Printed` to recognize only printed text."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "headers  = {'Ocp-Apim-Subscription-Key': subscription_key}\nparams   = {'mode' : 'Handwritten'}\ndata     = {'url': image_url}\nresponse = requests.post(text_recognition_url, headers=headers, params=params, json=data)\nresponse.raise_for_status()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "The text recognition service does not return the recognized text by itself. Instead, it returns immediately with an \"Operation Location\" URL in the response header that must be polled to get the result of the operation."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "operation_url = response.headers[\"Operation-Location\"]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "After obtaining the `operation_url`, you can query it for the analyzed text. The following lines of code implement a polling loop in order to wait for the operation to complete. Notice that the polling is done via an HTTP `GET` method instead of `POST`."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import time\n\nanalysis = {}\nwhile not \"recognitionResult\" in analysis:\n    response_final = requests.get(response.headers[\"Operation-Location\"], headers=headers)\n    analysis       = response_final.json()\n    time.sleep(1)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Next, the recognized text along with the bounding boxes can be extracted as shown in the following line of code. An important point to note is that the handwritten text recognition API returns bounding boxes as **polygons** instead of **rectangles**. Each polygon is _p_ is defined by its vertices specified using the following convention:\n\n<i>p</i> = [<i>x</i><sub>1</sub>, <i>y</i><sub>1</sub>, <i>x</i><sub>2</sub>, <i>y</i><sub>2</sub>, ..., <i>x</i><sub>N</sub>, <i>y</i><sub>N</sub>]"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "polygons = [(line[\"boundingBox\"], line[\"text\"]) for line in analysis[\"recognitionResult\"][\"lines\"]]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Finally, the recognized text can be overlaid on top of the original image using the extracted polygon information. Notice that `matplotlib` requires the vertices to be specified as a list of tuples of the form:\n\n<i>p</i> = [(<i>x</i><sub>1</sub>, <i>y</i><sub>1</sub>), (<i>x</i><sub>2</sub>, <i>y</i><sub>2</sub>), ..., (<i>x</i><sub>N</sub>, <i>y</i><sub>N</sub>)]\n\nand the post-processing code transforms the polygon data returned by the service into the form required by `matplotlib`."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from matplotlib.patches import Polygon\n\nplt.figure(figsize=(15,15))\n\nimage  = Image.open(BytesIO(requests.get(image_url).content))\nax     = plt.imshow(image)\nfor polygon in polygons:\n    vertices = [(polygon[0][i], polygon[0][i+1]) for i in range(0,len(polygon[0]),2)]\n    text     = polygon[1]\n    patch    = Polygon(vertices, closed=True,fill=False, linewidth=2, color='y')\n    ax.axes.add_patch(patch)\n    plt.text(vertices[0][0], vertices[0][1], text, fontsize=20, va=\"top\")\n_ = plt.axis(\"off\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Wrapping it up\nYes! You did it! Great job on exploring some Azure cognitive services you can leverage in all your on-going software endeavours!\n![Celebration](img/celebration.gif)\n\n"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python2",
      "display_name": "Python 2",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.15",
      "file_extension": ".py",
      "codemirror_mode": {
        "version": 2,
        "name": "ipython"
      }
    },
    "ms_docs_meta": {
      "author": "JuliaNik",
      "description": "Get information and code samples to help you quickly get started using Python and the Computer Vision API in Microsoft Cognitive Services.",
      "manager": "ytkuo",
      "ms.author": "juliakuz",
      "ms.date": "02/02/2018",
      "ms.service": "cognitive-services",
      "ms.technology": "computer-vision",
      "ms.topic": "article",
      "services": "cognitive-services",
      "title": "Computer Vision API Python quick start | Microsoft Docs",
      "titleSuffix": "Computer Vision API Python quick start | Microsoft Cognitive Services"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}